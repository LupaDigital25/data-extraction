{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Library versions from pip:\n",
      "requests: Version: 2.32.3\n",
      "csv: None\n",
      "json: None\n",
      "hashlib: None\n",
      "os: None\n",
      "pandas: Version: 2.2.3\n"
     ]
    }
   ],
   "source": [
    "# local files\n",
    "import variables as var\n",
    "\n",
    "# libraries\n",
    "import requests\n",
    "import csv\n",
    "import json\n",
    "import hashlib\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import subprocess\n",
    "print(\"\\nLibrary versions from pip:\")\n",
    "libraries = ['requests', 'csv', 'json', 'hashlib', 'os', 'pandas']\n",
    "for lib in libraries:\n",
    "    version = subprocess.run(['pip', 'show', lib], capture_output=True, text=True)\n",
    "    version_info = next((line for line in version.stdout.splitlines() if line.startswith(\"Version\")), None)\n",
    "    print(f\"{lib}: {version_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def short_hash(text, length=4):\n",
    "    \"\"\"\n",
    "    Generate a shortened MD5 hash of the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to hash.\n",
    "        length (int): The length of the hash to return.\n",
    "\n",
    "    Returns:\n",
    "        str: A substring of the MD5 hash of the input text, truncated to the specified length.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the specified length is greater than the length of the full hash.\n",
    "    \"\"\"\n",
    "    hash_object = hashlib.md5(text.encode('utf-8'))\n",
    "    return hash_object.hexdigest()[:length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_apirequest(api, savename):\n",
    "    \"\"\"\n",
    "    Fetches data from the given API endpoint, processes the JSON lines, and writes the 'timestamp' and 'url' fields to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        api (str): The API endpoint URL to fetch data from.\n",
    "\n",
    "    Raises:\n",
    "        requests.exceptions.RequestException: If there is an issue with the HTTP request.\n",
    "        json.JSONDecodeError: If there is an issue with decoding a JSON line.\n",
    "\n",
    "    Writes:\n",
    "        A CSV file named 'urls.csv' with the following columns:\n",
    "        - 'timestamp': The timestamp from the JSON data.\n",
    "        - 'url': The URL from the JSON data.\n",
    "    \"\"\"\n",
    "    response = requests.get(api)\n",
    "    with open(f'{savename}.csv', 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['timestamp', 'url'])\n",
    "        raw_data = response.text.splitlines()\n",
    "        for line in raw_data:\n",
    "            if line.strip():\n",
    "                try:\n",
    "                    # Parse each JSON line individually\n",
    "                    json_obj = json.loads(line)\n",
    "                    timestamp = json_obj.get('timestamp', '')\n",
    "                    url = json_obj.get('url', '')\n",
    "                    # Write to CSV\n",
    "                    csv_writer.writerow([timestamp, url])\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Skipping malformed JSON: {line}\")\n",
    "\n",
    "def do_api_requests(link, source):\n",
    "    \"\"\"\n",
    "    Perform API requests to retrieve archived URLs and timestamps from the Arquivo.pt service.\n",
    "\n",
    "    This function constructs a list of API request URLs with different parameters and iterates over them,\n",
    "    processing each request and saving the results to a specified file.\n",
    "\n",
    "    Args:\n",
    "        link (str): The base URL to be queried in the API requests.\n",
    "        source (str): A string identifier for the source of the URLs, used in naming the output files.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    apis_formats = [\n",
    "        f\"https://arquivo.pt/wayback/cdx?url={link}*&output=json&filter==mime:text/html&fields=url,timestamp\"#,\n",
    "        #f\"https://arquivo.pt/wayback/cdx?url={link}*&output=json&filter==mime:text/html&fields=url,timestamp&to={2005}&limit=1500\",\n",
    "        #f\"https://arquivo.pt/wayback/cdx?url={link}*&output=json&filter==mime:text/html&fields=url,timestamp&to={2010}&limit=3000\",\n",
    "        #f\"https://arquivo.pt/wayback/cdx?url={link}*&output=json&filter==mime:text/html&fields=url,timestamp&from={2010}\",\n",
    "        #f\"https://arquivo.pt/wayback/cdx?url={link}*&output=json&filter==mime:text/html&fields=url,timestamp&from={2015}&limit=20000\",\n",
    "        #f\"https://arquivo.pt/wayback/cdx?url={link}*&output=json&filter==mime:text/html&fields=url,timestamp&from={2020}&limit=10000\",\n",
    "    ]\n",
    "    for i, api in enumerate(apis_formats):\n",
    "        savename = f\"data/urls/{source}_{short_hash(link)}_{i}\"\n",
    "        if os.path.isfile(f\"{savename}.csv\"):\n",
    "            print(f\"Skipping saved file {savename}.\")\n",
    "        else:\n",
    "            process_apirequest(api, savename)\n",
    "            print(f\"Processed {savename}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping saved file data/urls/RTP_d43a_0.\n",
      "Skipping saved file data/urls/RTP_d02b_0.\n",
      "Skipping saved file data/urls/RTP_0a5c_0.\n",
      "Skipping saved file data/urls/RTP_e6de_0.\n",
      "Skipping saved file data/urls/RTP_3710_0.\n",
      "Skipping saved file data/urls/RTP_4e69_0.\n",
      "Skipping saved file data/urls/RTP_ad94_0.\n",
      "Skipping saved file data/urls/RTP_7bd2_0.\n",
      "Skipping saved file data/urls/RTP_f020_0.\n",
      "Skipping saved file data/urls/Público_1723_0.\n",
      "Skipping saved file data/urls/Público_5a7a_0.\n",
      "Skipping saved file data/urls/Público_bdda_0.\n",
      "Skipping saved file data/urls/Público_97ae_0.\n",
      "Skipping saved file data/urls/Público_6d80_0.\n",
      "Skipping saved file data/urls/Público_a569_0.\n",
      "Skipping saved file data/urls/Público_3e1d_0.\n",
      "Skipping saved file data/urls/Público_1c93_0.\n",
      "Skipping saved file data/urls/Público_0dcd_0.\n",
      "Skipping saved file data/urls/Público_7a4e_0.\n",
      "Skipping saved file data/urls/Público_b6ba_0.\n",
      "Skipping saved file data/urls/Correio da Manhã_c8ca_0.\n",
      "Skipping saved file data/urls/Correio da Manhã_d735_0.\n",
      "Skipping saved file data/urls/Correio da Manhã_043a_0.\n",
      "Skipping saved file data/urls/Correio da Manhã_bec9_0.\n",
      "Skipping saved file data/urls/Correio da Manhã_b619_0.\n",
      "Skipping saved file data/urls/Correio da Manhã_a238_0.\n",
      "Skipping saved file data/urls/Correio da Manhã_fa28_0.\n",
      "Skipping saved file data/urls/Correio da Manhã_ee87_0.\n",
      "Skipping saved file data/urls/Correio da Manhã_c7d6_0.\n",
      "Skipping saved file data/urls/SAPO_9a32_0.\n",
      "Skipping saved file data/urls/SAPO_5033_0.\n",
      "Skipping saved file data/urls/SAPO_962c_0.\n",
      "Skipping saved file data/urls/SAPO_83e6_0.\n",
      "Skipping saved file data/urls/SAPO_2b66_0.\n",
      "Skipping saved file data/urls/SAPO_a2e8_0.\n",
      "Skipping saved file data/urls/SAPO_7c38_0.\n",
      "Skipping saved file data/urls/SAPO_dbbe_0.\n",
      "Skipping saved file data/urls/SAPO_937f_0.\n",
      "Skipping saved file data/urls/SAPO_6161_0.\n",
      "Skipping saved file data/urls/SAPO_4d0c_0.\n",
      "Skipping saved file data/urls/SAPO_ab99_0.\n",
      "Skipping saved file data/urls/SAPO_0f27_0.\n",
      "Skipping saved file data/urls/O Mirante_2442_0.\n",
      "Skipping saved file data/urls/SAPO_fc04_0.\n",
      "Skipping saved file data/urls/SAPO_9d26_0.\n",
      "Skipping saved file data/urls/AEIOU_3854_0.\n",
      "Skipping saved file data/urls/AEIOU_c2e4_0.\n",
      "Skipping saved file data/urls/AEIOU_e2a1_0.\n",
      "Skipping saved file data/urls/SAPO_cc87_0.\n",
      "Skipping saved file data/urls/SAPO_d666_0.\n",
      "Skipping saved file data/urls/SAPO_a4a1_0.\n",
      "Skipping saved file data/urls/SAPO_f5b5_0.\n",
      "Skipping saved file data/urls/SAPO_b9f4_0.\n",
      "Skipping saved file data/urls/Diário de Notícias_835c_0.\n",
      "Skipping saved file data/urls/Diário de Notícias_8f06_0.\n",
      "Skipping saved file data/urls/IOL_c462_0.\n",
      "Skipping saved file data/urls/IOL_a139_0.\n",
      "Skipping saved file data/urls/IOL_91a1_0.\n",
      "Skipping saved file data/urls/Dinheiro Vivo_aa6c_0.\n",
      "Skipping saved file data/urls/SAPO_68be_0.\n",
      "Skipping saved file data/urls/SAPO_9d26_0.\n",
      "Skipping saved file data/urls/Observador_783c_0.\n",
      "Skipping saved file data/urls/Observador_58b7_0.\n",
      "Skipping saved file data/urls/Observador_ab9f_0.\n",
      "Skipping saved file data/urls/Observador_bb46_0.\n",
      "Skipping saved file data/urls/Observador_913f_0.\n",
      "Skipping saved file data/urls/Observador_b18d_0.\n",
      "Skipping saved file data/urls/Observador_9228_0.\n",
      "Skipping saved file data/urls/Record_788d_0.\n",
      "Skipping saved file data/urls/Record_6614_0.\n",
      "Skipping saved file data/urls/Record_305f_0.\n",
      "Skipping saved file data/urls/TSF_b254_0.\n",
      "Skipping saved file data/urls/Lusa_7c0b_0.\n",
      "Skipping saved file data/urls/Lusa_5ecb_0.\n",
      "Skipping saved file data/urls/Lusa_f536_0.\n",
      "Skipping saved file data/urls/Lusa_6941_0.\n",
      "Skipping saved file data/urls/Lusa_1915_0.\n",
      "Skipping saved file data/urls/Lusa_b29c_0.\n",
      "Skipping saved file data/urls/Lusa_7e4e_0.\n",
      "Skipping saved file data/urls/Lusa_4d02_0.\n",
      "Skipping saved file data/urls/Notícias ao Minuto_4ac1_0.\n",
      "Skipping saved file data/urls/Notícias ao Minuto_a076_0.\n",
      "Skipping saved file data/urls/Notícias ao Minuto_69cb_0.\n",
      "Skipping saved file data/urls/Notícias ao Minuto_6983_0.\n",
      "Skipping saved file data/urls/Notícias ao Minuto_d8cd_0.\n",
      "Skipping saved file data/urls/Notícias ao Minuto_4d1e_0.\n",
      "Skipping saved file data/urls/Notícias ao Minuto_70d9_0.\n",
      "Skipping saved file data/urls/Notícias ao Minuto_801a_0.\n",
      "Skipping saved file data/urls/Notícias ao Minuto_ac6e_0.\n",
      "Skipping saved file data/urls/Notícias ao Minuto_ba05_0.\n",
      "Skipping saved file data/urls/IOL_cd0e_0.\n",
      "Skipping saved file data/urls/SIC Notícias_bf94_0.\n",
      "Skipping saved file data/urls/SIC Notícias_9660_0.\n",
      "Skipping saved file data/urls/SIC Notícias_7f54_0.\n",
      "Skipping saved file data/urls/SIC Notícias_ddf9_0.\n",
      "Skipping saved file data/urls/SIC Notícias_b633_0.\n",
      "Skipping saved file data/urls/SIC Notícias_ea2c_0.\n",
      "Skipping saved file data/urls/SIC Notícias_3f4d_0.\n",
      "Skipping saved file data/urls/Jornal de Negócios_61b7_0.\n",
      "Skipping saved file data/urls/Jornal de Negócios_fe43_0.\n",
      "Skipping saved file data/urls/Jornal de Negócios_91fc_0.\n",
      "Skipping saved file data/urls/Jornal de Negócios_e791_0.\n",
      "Skipping saved file data/urls/Jornal de Negócios_d6be_0.\n",
      "Skipping saved file data/urls/Jornal de Notícias_7492_0.\n",
      "Skipping saved file data/urls/SAPO_5b76_0.\n",
      "Skipping saved file data/urls/SAPO_dbcb_0.\n",
      "Skipping saved file data/urls/SAPO_d2c7_0.\n",
      "Skipping saved file data/urls/SAPO_2a2a_0.\n",
      "Skipping saved file data/urls/SAPO_77c9_0.\n",
      "Skipping saved file data/urls/SAPO_bdd3_0.\n",
      "Skipping saved file data/urls/Expresso_6441_0.\n",
      "Skipping saved file data/urls/Expresso_d365_0.\n",
      "Skipping saved file data/urls/Expresso_2506_0.\n",
      "Skipping saved file data/urls/Expresso_c298_0.\n",
      "Skipping saved file data/urls/Expresso_3f4d_0.\n",
      "Skipping saved file data/urls/Expresso_be95_0.\n",
      "Skipping saved file data/urls/Expresso_a955_0.\n",
      "Skipping saved file data/urls/Expresso_8293_0.\n",
      "Skipping saved file data/urls/Expresso_f87a_0.\n",
      "Skipping saved file data/urls/Expresso_dfbe_0.\n",
      "Skipping saved file data/urls/Expresso_acbf_0.\n",
      "Skipping saved file data/urls/CNN Portugal_40a2_0.\n",
      "Skipping saved file data/urls/CNN Portugal_bf3a_0.\n",
      "Skipping saved file data/urls/CNN Portugal_40da_0.\n",
      "Skipping saved file data/urls/NiT_f62a_0.\n",
      "Skipping saved file data/urls/NiT_7bba_0.\n",
      "Skipping saved file data/urls/NiT_4471_0.\n",
      "Skipping saved file data/urls/NiT_d4df_0.\n",
      "Skipping saved file data/urls/NiT_3859_0.\n",
      "Skipping saved file data/urls/NiT_2bac_0.\n",
      "Skipping saved file data/urls/NiT_958e_0.\n",
      "Skipping saved file data/urls/NiT_6849_0.\n",
      "Skipping saved file data/urls/NiT_5ed5_0.\n",
      "Skipping saved file data/urls/NiT_17e2_0.\n",
      "Skipping saved file data/urls/NiT_0fd6_0.\n"
     ]
    }
   ],
   "source": [
    "# Perform API requests for each news source\n",
    "for link, source in var.news_sources:\n",
    "    do_api_requests(link, source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all CSV files into a single DataFrame\n",
    "data_folder = 'data/urls'\n",
    "dfs = []\n",
    "for file in os.listdir(data_folder):\n",
    "    if file.endswith('.csv'):\n",
    "        df = pd.read_csv(f'{data_folder}/{file}')\n",
    "        df[\"source\"] = file.split('_')[0]\n",
    "        dfs.append(df)\n",
    "df = pd.concat(dfs)\n",
    "\n",
    "# Remove duplicates and sort by timestamp\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.sort_values(by='timestamp', inplace=True)\n",
    "\n",
    "# Filter out URLs that have been archived multiple times in the same month, after June 2010\n",
    "df['timestamp_prefix'] = df['timestamp'].astype(str).str[:6]\n",
    "df01 = df[df['timestamp_prefix'] <= '201006']\n",
    "df02 = df[df['timestamp_prefix'] > '201006']\n",
    "df02 = df02.drop_duplicates(subset=['timestamp_prefix', \"url\"], keep='last')\n",
    "df = pd.concat([df01, df02])\n",
    "df = df.drop(columns=['timestamp_prefix'])\n",
    "\n",
    "# Add archive URLs\n",
    "def get_archive(timestamp, url):\n",
    "    return f\"https://arquivo.pt/noFrame/replay/{timestamp}/{url}\"\n",
    "df['archive'] = df.apply(lambda row: get_archive(row['timestamp'], row['url']), axis=1)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df['id'] = df.index\n",
    "df.to_csv('data/urls.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3056418, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "source\n",
       "Correio da Manhã      471684\n",
       "RTP                   358896\n",
       "Notícias ao Minuto    325957\n",
       "SAPO                  310735\n",
       "SIC Notícias          293420\n",
       "Público               261402\n",
       "Expresso              157668\n",
       "Jornal de Negócios    139033\n",
       "IOL                   135812\n",
       "Observador            111529\n",
       "Record                 96406\n",
       "AEIOU                  81705\n",
       "NiT                    79326\n",
       "Lusa                   75907\n",
       "O Mirante              55556\n",
       "Dinheiro Vivo          49116\n",
       "Diário de Notícias     16317\n",
       "Jornal de Notícias     14424\n",
       "CNN Portugal           12827\n",
       "TSF                     8698\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the shape of the final DataFrame\n",
    "print(df.shape)\n",
    "\n",
    "# Print the number of URLs per source (avg. ~20 news/day)\n",
    "df[\"source\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condainfo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
